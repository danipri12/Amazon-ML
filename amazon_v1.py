# -*- coding: utf-8 -*-
"""amazon_v1.ipynb 

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ai6y1_GYVrQpduz_04B9Bqhk-BSR3aO8
"""

# upload the csv file
from google.colab import files
uploaded = files.upload()

# libraries
import pandas as pd
import numpy as np

from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.metrics.pairwise import cosine_similarity

# load the csv
df = pd.read_csv("amazon.csv")

# keep select features
df = df[[
    "category",
    "discounted_price",
    "actual_price",
    "discount_percentage",
    "rating",
    "rating_count",
    "about_product"
]]

# look at first few products of df
df.head()

# extract the main category
df["main_category"] = (
    df["category"].str.split("|").str[0]
)

# clean numeric features
numeric_features = [
    "discounted_price",
    "actual_price",
    "discount_percentage",
    "rating",
    "rating_count"
]

# remove non-numeric characters and convert to float
for col in numeric_features:
    df[col] = (
        df[col]
        .astype(str)
        .str.replace(r"[^\d.]", "", regex=True)
        .replace("", np.nan)
        .astype(float)
    )

# fill missing numerics with the median
imputer = SimpleImputer(strategy="median")
numeric_imputed = imputer.fit_transform(df[numeric_features])

# analyze the distribution
print(df["rating_count"].describe(percentiles=[0.5, 0.9, 0.95, 0.99]))

# dataset analysis
# print the mean and median rating_count in each category
category_stats = df.groupby("main_category")["rating_count"].agg(["mean", "median", "count"]).sort_values("mean", ascending=False)
print(category_stats.head(10))

# numeric distributions
df[numeric_features].describe()

# print category counts
print(df['category'].value_counts().head(10))

# cap extreme outliers
cap_value = df["rating_count"].quantile(0.99)

df["rating_count_capped"] = np.minimum(
    df["rating_count"],
    cap_value
)

print("Capping rating_count at:", cap_value)

scaler = StandardScaler()

# exclude the target variable for scaling
numeric_scaled = scaler.fit_transform(numeric_imputed[:, :-1])

# encode the categories
df["main_category"] = df["main_category"].fillna("Unknown").astype(str)

encoder = OneHotEncoder(
    sparse_output=False,
    handle_unknown="ignore")
category_encoded = encoder.fit_transform(df[["main_category"]])

# add llm embeddings for product descriptions
!pip install -q sentence-transformers
from sentence_transformers import SentenceTransformer

df["about_product"] = df["about_product"].fillna("").astype(str)

model = SentenceTransformer("all-MiniLM-L6-v2")
product_embeddings = model.encode(
    df["about_product"].tolist(),
    show_progress_bar=True
)

# create the final feature matrix
X_final = np.hstack([
    numeric_scaled,
    category_encoded,
    product_embeddings
])

# check for NaNs still present
print("NaNs in X_final:", np.isnan(X_final).sum())

# compute the similarity matrix
similarity_matrix = cosine_similarity(X_final)

# log-transform the target variable
# note: log1p(x) = log(1 + x)
y_raw = df["rating_count"]
df["log_rating_count"] = np.log1p(y_raw.fillna(y_raw.median()))
y_log = df["log_rating_count"].values

# predict rating_count using top-k neighbors and threshold
def predict_log_rating(sim_matrix, y_log, k=10, min_sim=0.4):
    preds = []

    for i in range(len(y_log)):
        sims = sim_matrix[i].copy()
        sims[i] = 0  # exclude self

        mask = sims >= min_sim
        sims = sims[mask]
        y_vals = y_log[mask]

        if len(sims) == 0:
            preds.append(np.mean(y_log))
            continue

        # take top-k neighbors
        top_k_idx = np.argsort(sims)[-k:]
        top_k_sims = sims[top_k_idx]
        top_k_y = y_vals[top_k_idx]

        # compute the weighted average
        preds.append(np.sum(top_k_sims * top_k_y) / np.sum(top_k_sims))

    return np.array(preds)

# convert back from log scale
df["predicted_rating_count"] = np.expm1(predict_log_rating(similarity_matrix, y_log))

df["predicted_log"] = np.log1p(df["predicted_rating_count"])

# global percentile
df["global_percentile"] = df["predicted_log"].rank(pct=True)

# category percentile
df["category_percentile"] = (
    df.groupby("main_category")["predicted_log"]
    .rank(pct=True)
)

# category z-score
df["category_zscore"] = (
    df.groupby("main_category")["predicted_log"]
    .transform(lambda x: (x - x.mean()) / x.std())
)

# normalized log strength
log_min = df["predicted_log"].min()
log_max = df["predicted_log"].max()

df["log_strength"] = (
    (df["predicted_log"] - log_min) /
    (log_max - log_min)
)

# final market potential score
df["market_potential_score"] = (
    0.4 * df["category_percentile"] +
    0.3 * df["global_percentile"] +
    0.3 * df["log_strength"]
)

# top 5 startup opportunities
top_5 = (
    df.sort_values("market_potential_score", ascending=False)
      .head(5)
)

top_5[[
    "main_category",
    "rating_count",
    "predicted_rating_count",
    "market_potential_score"
]]

# visualize using matplot and seaborn
import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(10,6))
sns.scatterplot(
    x="rating_count",
    y="predicted_rating_count",
    hue="main_category",
    size = "category_percentile",
    data=df,
    alpha=0.6
)
plt.title("Predicted vs Actual Rating Count by Category")
plt.xscale("log")
plt.yscale("log")
plt.xlabel("Actual rating_count (log scale)")
plt.ylabel("Predicted rating_count (log scale)")
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
plt.show()

# user input for testing a product for predicted rating count and market success

categories = sorted(df["main_category"].unique())

print("Choose a product category by number:\n")
for i, cat in enumerate(categories):
    print(f"{i}: {cat}")

cat_index = int(input("\nEnter category number: "))
category_input = categories[cat_index]

discounted_price_input = float(input("Enter discounted price: "))
actual_price_input = float(input("Enter actual price: "))
discount_percentage_input = float(input("Enter discount percentage: "))
rating_input = float(input("Enter expected rating (1-5): "))
about_product_input = input("Enter product description: ")

# encode category properly
cat_df = pd.DataFrame([[category_input]], columns=["main_category"])
cat_array = encoder.transform(cat_df)

# numeric
num_array = np.array([
    discounted_price_input,
    actual_price_input,
    discount_percentage_input,
    rating_input
]).reshape(1, -1)

num_scaled = scaler.transform(num_array)

# embedding
product_emb = model.encode([about_product_input])

# combine
X_hypo = np.hstack([num_scaled, cat_array, product_emb])

# similarity
similarities = cosine_similarity(X_hypo, X_final)[0]

# prediction
def predict_single(sim_vector, y_log, k=15, min_sim=0.35):
    sims = sim_vector.copy()
    mask = sims >= min_sim
    sims = sims[mask]
    y_vals = y_log[mask]

    if len(sims) == 0:
        return np.mean(y_log)

    top_k_idx = np.argsort(sims)[-k:]
    return np.sum(sims[top_k_idx] * y_vals[top_k_idx]) / np.sum(sims[top_k_idx])

predicted_log = predict_single(similarities, y_log)
predicted_rating_count = np.expm1(predicted_log)

# compute metrics
global_percentile = (df["predicted_log"] < predicted_log).mean()

category_mask = df["main_category"] == category_input
category_percentile = (
    (df.loc[category_mask, "predicted_log"] < predicted_log).mean()
)

cat_mean = df.loc[category_mask, "predicted_log"].mean()
cat_std = df.loc[category_mask, "predicted_log"].std()
category_zscore = (predicted_log - cat_mean) / cat_std

log_strength = (
    (predicted_log - log_min) /
    (log_max - log_min)
)

market_score = (
    0.4 * category_percentile +
    0.3 * global_percentile +
    0.3 * log_strength
)

print("\nProduct Analysis: ")
print("Category:", category_input)
print("Predicted rating count:", round(predicted_rating_count))
print("Global percentile:", round(global_percentile,3))
print("Category percentile:", round(category_percentile,3))
print("Category z-score:", round(category_zscore,3))
print("Market potential score:", round(market_score,3))

# print interpretations to the scores
print("\nInterpretation: ")

if category_percentile > 0.9:
    print("Top 10% product within this category.")
elif category_percentile > 0.75:
    print("Strong product within this category.")
elif category_percentile > 0.5:
    print("Above average within this category.")
else:
    print("Below median performance for this category.")

if global_percentile > 0.9:
    print("Very strong product across entire marketplace.")
elif global_percentile > 0.75:
    print("Strong globally.")
elif global_percentile > 0.5:
    print("Above average globally.")
else:
    print("Would struggle to compete globally.")

# to test multiple products at once
def analyze_product():
    print("\nChoose a product category by number:\n")

    # use trained encoder
    categories = encoder.categories_[0]

    for i, cat in enumerate(categories):
        print(f"{i}: {cat}")

    try:
        category_idx = int(input("\nEnter category number: "))
        category = categories[category_idx]
    except:
        print("Invalid category number.")
        return

    try:
        discounted_price = float(input("Enter discounted price: "))
        actual_price = float(input("Enter actual price: "))
        discount_percentage = float(input("Enter discount percentage: "))
        rating = float(input("Enter expected rating (1-5): "))
    except:
        print("Invalid numeric input.")
        return

    description = input("Enter product description: ")

    # create features
    user_numeric = np.array([[discounted_price, actual_price, discount_percentage, rating]])
    user_numeric_scaled = scaler.transform(user_numeric)

    user_category_encoded = encoder.transform([[category]])

    user_text_embedding = model.encode([description])

    user_features = np.hstack([
        user_numeric_scaled,
        user_category_encoded,
        user_text_embedding
    ])

    # predict similarity
    similarities = cosine_similarity(user_features, X_final)[0]

    predicted_log = predict_single_rating(similarities, y_log)
    predicted_count = np.expm1(predicted_log)

    # relative metrics
    global_percentile = (df["rating_count"] < predicted_count).mean()

    category_df = df[df["main_category"] == category]
    category_percentile = (category_df["rating_count"] < predicted_count).mean()

    category_mean = category_df["rating_count"].mean()
    category_std = category_df["rating_count"].std()
    z_score = (predicted_count - category_mean) / category_std

    print("\nProduct Analysis:")
    print(f"Category: {category}")
    print(f"Predicted rating count: {int(predicted_count)}")
    print(f"Global percentile: {global_percentile:.3f}")
    print(f"Category percentile: {category_percentile:.3f}")
    print(f"Category z-score: {z_score:.3f}")

    if category_percentile > 0.8:
        print("Extremely strong within this category.")
    elif category_percentile > 0.6:
        print("Strong within this category.")
    elif category_percentile > 0.4:
        print("Around average within this category.")
    else:
        print("Weak within this category.")


# loop until break
while True:
    analyze_product()
    again = input("\nTest another product? (y/n): ")
    if again.lower() -= "n":
        print("Done testing.")
        break
